---
layout: post
title: FreeBSD
---

## Init Main

So freebsd calls `mi_startup` as its first common entry point it seems, at `sys/kern/init_main.c`. Before that, it may be booted from something like GRUB or simply an EFI stub. I think it might also have an MBR stub too.

There, it does a few things. I noticed the first thing it does is to declare pointers to `sysinit` structs. Some double pointers. I think those are meant to be system init tasks, some of which are tabulated. A lot of it is traversing through the sysinit tables' primary and secondary keys (SQL like?), and sorting them, then calling `(*((*sipp)->func))((*sipp)->udata);` to initialise that system. Its done in this manner to make it easy to plug and play a subsystem like the VMM or cpu. And maybe allow dynamic loading of arch dependent subsystems.

`sysinit` is just:

```c
struct sysinit {
 void (*func) (void *arg);
 void *data;
};
```

So a function pointer that takes in some unformatted arg. And a piece of data. The actual data looks like `sysinit_data`, which is a struct containing the filename, debug info, msb, lsb, etc. Of a certain value (in a config file?). `SYSINT()` is called in many places, presumably to start each core subsystem.

## Memory Management

There is arch dependent and arch independent code for it. Normally all arches have a hardware MMU with a set of common page attributes. Like dirty, paged on disk, protected, ro, rdwrite, executable.

Looks like each process (including the kernel) has its own `vm_map`. And a set of `vm_map_entry`. Each `entry` tells you a specific address range being used for that map. This is useful in the kernel for assigning memory ranges for subsystems.

One security feature is that the first 4K-8K of RAM is actually reserved for the kernel. Prob the text or some known data. This eases debugging as well. If you try to null deref via privilege escalation, this invokes a page fault. Since you know what data is there as well, you shouldn't leak any sensitive data in case the code somehow manages to reference a null pointer.

When they say "memory allocator" I think they mostly mean dynamic memory allocation. So not so much as atomic as page allocation itself, but how to allocate general free "areas". An area could be a page. Or a set of pages. Or half a page. But usually prob multiples of the min page size.

### Linked List of VMAP Entry?

So you have a linked list of `vm_map_entry`. With a `vm_map` entry point, which there should be one of for a `vm_space`.

Each `entry` should point to a vnode in a 1:m manner. And each `vnode` should point to a linked list of `vm_page`. Which are prob 4K-2M in size. The `vm_page` are prob just u64 pointers to the actual base addr of the physical frame containing the start of the data.

I dunno why they do it like this. What about the page table for their arch? Do they use that in accordance to this? That for the MMU, and this to keep track of what pages are being used and shared? So you dont have to run through the page table each time to analyse what is being used and whats not being used or shared. It seems kinda efficient I guess, for a GC to come and kick out long time no used pages.

Note each `entry` describes a cont virtual addr space. I think they are also ordered as well. But the vnodes that back them up maybe not be ordered or contiguous. What does 'region' mean? It means a collection of data/memory that is being treated the same 'way'. So theres some set of attributes that defines those pages. Maybe read-only, etc. It can also be demand paged from disk all at once? You also have an `offset` that tells you where the start of the region is in the `vm_object`. So when you try to read/write to it, you always add the offset. I think read/writing to the `vm_object` is handled by its own method that scours for the specific page from the page table and writes the bytes and size to it (as long as it doesnt overshoot the page?).

What is `vm_pmap`? Physical map of frames for the virtual map entries?

Wait.. No its actually more like a BST it seems. So you still have a head as normal. But you have up to 2 children. Its sorted by the start addr Im pretty sure. I dont think they can overlap. So searching for a specific region is log(N) usually, in extra RAM accesses using implcit kernelspace MMU. And I guess yo ucould always plug it out and put in a new model, if the subsystems are modular enough.

### Page Fault Handler

So you have either a R/W/E instruction on that virtual addr. Prob byte, word, or vector. Then you go into the handler and you lookup a tree. You use tarjan and sleator's algo to reorder the tree and find strongly connected components. This results in the most recent 'items' at the top of the tree. But there may be some lock contention. Its good for most cases where recently used items should be at the top. I.e. you move the faulted address 'item' to the top of the tree fast, so if you fault again on that, it will be close.

Wait ok. So you need to identify the `vmspace` struct for the faulting process. This should be dynamically alloc'd in the kernel. And you should have a ref or pointer to it. Next find the BST for the `vm_map_entries`.

You calc the actual offset within the `vm_object` as `vm_object_offset = fault_addr - start_addr + offset`. So you get rid of the virtual addr. The `vm_object` should then allocate a `vm_page`, and map it to the faulting location. That would be handled by arch dependent code, maybe some asm and fencing too. After the map into vm_object, the handler will reexecute the faulting instruction. So I think it eret to it without adding 4B like it usually does with jal + ret.

### VM Object

A `vm_page` can only be mapped to exactly one `vm_object`. But multiple vm_entries can point to the same vm_object to access that page.

### Per Process

Apparently ELF also specifies how much you need for the runtime stack? Of an executable. Err. So Freebsd defines 4 regions for a process's initial virtual addr space. Text, Initialised Data, Uninitalised, and Runtime Stack Size.

The ELF may also require dynamic libraries to be linked to it before running. Generally the .dynamic section should have n entries for n deps that should be linked. Generally the deps are linked at random addrs or have their absolute addrs patched. Its slower for sure but seems more modular and safe. Libc could be linked at runtime, and sometimes it prob is if you dont care about the initialisation step.

The process of finding the libs to be linked uses an executable `/libexec/ld-elf.so`. This shared lib actually has a bunch of startup code like `.init` that is run before control passes to the actual program entry (prob `_start` which initialises the C runtime args and env then goes to `_main`).
