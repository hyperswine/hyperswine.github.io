---
layout: post
title: AGVN System v1
---

## Summary

The roman cycle -> one invents, rises to greatness, then is destroyed from within. Another picks up from thereon, rises to greatness, and is destroyed from within.

The zerubabel effect -> create, destroy, create and so on.

### True Freedom

A sense that one is truly free to make their own decisions. To live their own ways. Within certain boundaries determined by the nature of the universe.

- Not like now, where noone is free

### Permanent Progress

Continual, everlasting progression of culture and technology. Allowing individuals to more effectively and efficiently manipulate spacetime.

Ultimate goal => Truly understand the nature of the universe and become the master of it. </br></br>

## Design

### First Principles

Loose coupling -> easily upgrades when needed.

KISS -> each atomic component should be as simple and straightforward as possible. Complexity is built through layers and relationships between simpler components. Each component should have a primary idea for its level and a single responsibility for that level.

KEY IDEA -> model only for the next 10-20 years. Never more, never less. Things like "what happens if X eventually happens down the line" is very tempting and can be considered, but in practice no. Just No.

## Implementation Tools

### PHANTA REI

- **Rei** -> a systems an app level language.
- **Prei** (Package Rei) -> a strong package manager and auto config/build system that uses high level prei syntax.
- **Phanta** -> a modular, NLP based scripting and Object oriented prototyping system. Mainly for incorporating into rei based projects.

### Umbral Suite & Fabricator IDE

- **Editor** -> includes syntax highlighting and autoamtic annotations and macros and expansions akin to rust, ts-react, clion and intelliJ
- **FFX Layouter** -> akin to Qt Designer or Android Studio XML.
Git Wrapper Builtin -> builtin to the IDE, a high level git wrapper using reiversion.
- **Documenter Builtin** -> akin to zeal when you hover over a specific identifier or keyword. Like rust-analyzer as well with error squiggles and unused lines. Renders reidoc double slash comments on the fly.
- **Umbral Database** -> great Key-Value boosted (in RAM) database that is written to the disk (backup) but CPU-Cache Explotative and RAM-first. Good for high CPU cache server CPUs.

### Runtime & Environment

- **Neutron Quanta** -> Kernel/OS User Environment that is optimised for rei and quick iterations/hot reload programming (HRP), basically like flutter + vscode.
- **Reii** (Rei Interpreter) -> like node or python interpreters. Allows line by line/statement by statement interpretation when using a notebook or terminal and JIT for HRP. Allows us to run rei scripts easily to manage certain things in the environment, build systems like prei, intervaled scripts (IVS).
- **Ardaku DE** -> Highest Level GUI environment for Quanta based on NeutronWM, akin to Wayland bus architecture.

### Hardware

- **Neuromorph I** -> A neuromorphic chip that stores a large neural network (digraph nodes with weighted edges). Can be easily trained directly via writing the inputs to the Driver/MMIO
- **Server CPU** -> A high performance Server CPU with 128 Cores, superscalar (no VLIW), high cache for all levels. For highly parallel requests (mostly reads), since writes only need DMA from RAM to Neuromorph I's NRAM

### Possible Improvements

- **Quanta I** -> A quantum chip with many qcores. Can be used for high performance, low level simulations. High level simulations also possible. Useful for all kinds of forecasting, but for human use rather than internal machine use

## Models

The types of ways to represent the data that can be collected:

- general historical recounts
- precise contemporary statistics (human)
- precise contemporary statistics (physical)

Useful generalistic models and data representation:

- linear and nonlinear relationships between a bunch of input variables and an output variable of interest. Or vice-versa, how a specific input variable affects other variables
- unsupervised clustering, mostly nonlinear and high dimensional. When you dont quite "know what you want" in a chunk of data. So you pass it in and see what groupings it can find
- neural modeling of the data -> CNN, ANN, RNN
- self-labeling, applies to NN

### Rei Modeling

I want rei to have support for things like matrices and tensors in its standard library. Also support for FP8, SP12, FP16/BF16. This allows a lot of data science and ML to be easily coded from scratch without an external library like Eigen/Dense. Rei emphasises good code and software eng principles above all else. There is usually two ways of doing everything, a straightforward "just get it out there" way and a more optimal "lets refactor this into map-reduce-lambda and use a more optimal algorithm with a good space-time tradeoff".

- direct support for key and very generalistic models like clusters (k-nearest, spectral, hierarchical), principal component analysis, a very basic ANN, CNN and RNN and a bunch of hooks for your own custom functionality like the objective function, evolutionary and swarm algorithms, automl pipelines which use a certain interface (you have to implement a rei model that implements the interface)
- direct scripting/on the fly interpretation or compilation. Also notebooks akin to ipynb
- standard library support for reading csvs into matrices and applying functions to rows, cols or specific ranges using the `Range` trait
- standard library support for GPU based code. Can interface with neuromorph I and the GPU/GPGPU/TPU with rei like CUDA C++
- very clean and simple syntax like python and scala. Functions are first class components and can be passed directly with `&func` param arg and implicit `@template` which creates the function of type `func` every time you pass a new function type
- allows close integration with other styles of programming too. OOP, imperative (e.g. state machine), procedural (scripting), functional (closer mathematical formulations with less emphasis on state and rather passing a specific expression/formulation entirely every time)
- standard library support for reigraph style graphing of data. Based on a simplified matplotlib with render contexts and windows for similar vs different data contexts
- dont have to worry about speed or optimisation. Rei-std is highly optimised and anything built on it should be too. Just pass the `--release` flag for maximum optimisation given that your code is good

Best Support:

- allowing you to not only get models up fast like python, but also rely build more on the standard library than use separate libraries like tensorflow, pytorch, opencog, etc
- emphasis on using existing logic and building your own functionality either from another crate (that relies a lot on standard) or from scratch. Emphasis on `newer is better, the more personal the better`
- emphasis on good documentation generation, possible only if you know what you are truly doing. And you should use `reidoc` in all cases unless you really think your idea is better
- emphasis on ease of interfacing. With the Rei-AutoML library built on std, you can also create your own ML pipelines from data collection to practical usage, all in a single package/language
- emphasis on key algorithms like PCA, Clustering, AutoML, Neural Representations (NN, Graph NN for neuromorphic)

### Core Model

The Core AGVN System Model is the largest model. It is used to represent the universe in the most detailed way possible. As such a very large array of neurons and connections is required.

- training would take a very long time, though maybe a neuromorphic computer could speed up the process by representing the model more closely

Basically, if it exists, then we have to collect data on it. How we collect that data and how we then choose to represent it is a different story.

- for very abstract data, we simply just look at it through a human lens. And give it a categorical or numeric label according to what we think best represents it
- most of the time, an abstract feature can and should be broken down to smaller features. If possible, make them as atomic as possible and numeric, since the universe is very numeric, although many features that are useful to human progress and freedom are categorical and binary

### Submodels

If it would generate too much traffic in the core model to do absolutely everything, we can have submodels that are of smaller size and space representation.

- for example, all things to do with military defense and operations can be offset to a military model. Then the core model doesnt need to know about the military or just more general things like numbers and etc, rather than closely which troops are where and news on new weapons by adversaries, and how to best optimise for the military
- then these submodels can query the main model to request funding. Based on the core model's view of the universe/world, it then scales the request and returns a budget or a confirmation/rejection for a specific action to be taken

### Actions

Actions are immediate forms of policies. Whereas the core model is to make policies, the submodels can be made to optimal actions based on very specific data fed in rapidly.

- the tradeoff is that the local models will have to be very specific in its topic/field, like making economic decisions, education decisions, military decisions, etc
